<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on Kanishk</title>
    <link>kanishkg.github.io/project/</link>
    <description>Recent content in Projects on Kanishk</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Kanishk Gandhi</copyright>
    <lastBuildDate>Sun, 17 Dec 2017 18:29:35 +0530</lastBuildDate>
    <atom:link href="kanishkg.github.io/project/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Video Summarisation</title>
      <link>kanishkg.github.io/project/vsumm/</link>
      <pubDate>Sun, 17 Dec 2017 18:29:35 +0530</pubDate>
      
      <guid>kanishkg.github.io/project/vsumm/</guid>
      <description>&lt;p&gt;With an explosion in multimedia content production due to ever increasing reach of internet, content in the form of videos is becoming increasingly commonplace, becoming the preferred method for the purpose of delivering content.  Advent of social media and growth of video sharing websites, in particular Youtube, has only contributed to the increasing importance of videographic content. More content is uploaded to Youtube a day, than a person is capable of watching in his/her whole lifetime.   With the emergence of video content as an effective mode of information propagation, automating the process of summarization a video has become paramount.  Video Summarization, in  recent  times,  has  emerged  as  a  challenging  problem  in  the  field  of  machine  learning,  which aims at automatically evaluating the content of a video, and generating a summary with the most relevant content of the video.  Video summarization finds applications in generating highlights for sports events, trailers for movies and in general shortening video to the most relevant subsequences, allowing humans to browse large repository of videos efficiently.&lt;/p&gt;

&lt;p&gt;Video  summarization  is  a  challenging  problem  in  multiple  facets.    There  is  no  natural  ordering of video summaries.   Among a given set of video summaries,  the best representation of the original video is highly subjective. The general objective in modern literature for video summarization aims at producing a summary, typically 5%-15% of the whole video, consisting of the most informative content from the original video. The content is usually represented in the form of ”key frames”, or more appropriately as video skims. A good video summary depicts the synopsis of the original video, in a compact way depicting all important and relevant scenes/shots. Through this project, we review the major techniques in video summarization, and look at their performance on some recent datasets. Most of the work pertaining to this project can be found on &lt;a href=&#34;https://github.com/architsharma97/VideoSummarization/&#34; target=&#34;_blank&#34;&gt;github here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://home.iitk.ac.in/~kanishkg/Video_Summarization_Final_Report.pdf&#34; target=&#34;_blank&#34;&gt;Read More&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GANs for dehazing</title>
      <link>kanishkg.github.io/project/dehaze-gans/</link>
      <pubDate>Sun, 17 Dec 2017 18:19:40 +0530</pubDate>
      
      <guid>kanishkg.github.io/project/dehaze-gans/</guid>
      <description>&lt;p&gt;Single image dehazing is a complex and ill-posed task.  Hazing is attributed to dust,  fog and other environmental factors which severely degrades the images.  Image hazing is a function of depth, where the visual contrast  reduces  rapidly  as  the  depth  of  objects  in  the  images  increases.   From  a  perspective  of  vision,  this
severely impacts feature retrieval based tasks.  Degraded photos often lack visual appeal and offer poor visibility of  scene  contents.   Thus,  dehazing  images  forms  an  important  task  in  consumer  photography  and  a  crucial preprocessing step in vision tasks.  There has not been a lot of work in image dehazing in the domain of deep learning. Most of the deep learning approaches that exist for single image dehazing are based on multi-scale CNNs.&lt;/p&gt;

&lt;p&gt;We explore this problem from the perspective of Generative Adversarial Networks for this first time and experiment with different formulations of loss functions and stacking GANs.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://home.iitk.ac.in/~kanishkg/Conditional_GANs_for_Single_Image_Dehazing.pdf&#34; target=&#34;_blank&#34;&gt;Read More&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Natural Parameter Networks</title>
      <link>kanishkg.github.io/project/npnrnn/</link>
      <pubDate>Sun, 17 Dec 2017 18:06:49 +0530</pubDate>
      
      <guid>kanishkg.github.io/project/npnrnn/</guid>
      <description>&lt;p&gt;Neural networks (NNs) have seen a recent resurgence of interest due to empirical achievements on a wide range of supervised learning problems.  In their typical usage, neural networks are highly expressive models that can learn complex functions.  Neural Networks require a massive amount of data for training, so stochastic algorithms are generally used for training.  But, neural networks are prone to overfitting, especially when training data is insufficient.  Also there is no estimate of our  confidence  in  our  prediction.   Bayesian  methods  can  be  helpful  in  tackling  these  problems.&lt;/p&gt;

&lt;p&gt;Wang et al(2016) proposed one such application named Natural Parameters Networks in NIPS
2016. Natural parameter networks make use of the properties of the exponential families to efficiently train the neural networks using backpropagation, while maintaining the flexibility of the network i.e. allowing the modelling of data using distributions other than Gaussian.&lt;/p&gt;

&lt;p&gt;Our project was mainly aimed at implementing the Natural Parameter Networks for feed forward neural networks, and extending this framework for Recurrent Neural Networks.  The authors had not open sourced the code for Natural Parameter Networks.  This, we have implemented the Gaussian and Gamma NPN using tensorflow in python.  We also completed the mathematical details of NPN, as well as other model involving bayesian analysis in neural networks, like Probabilistic Back Propagation, Dropout Neural Networks, Bayesian Dark Knowledge, and Variational RNN.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;http://home.iitk.ac.in/~kanishkg/BML_Final_Report.pdf&#34; target=&#34;_blank&#34;&gt;Read More&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Artifacia Intern</title>
      <link>kanishkg.github.io/project/artifacia-intern/</link>
      <pubDate>Sun, 17 Dec 2017 17:51:40 +0530</pubDate>
      
      <guid>kanishkg.github.io/project/artifacia-intern/</guid>
      <description>

&lt;h2 id=&#34;the-art-of-learning-to-learn-at-artifacia-http-artifacia-com&#34;&gt;The art of learning to learn at &lt;a href=&#34;http://artifacia.com&#34; target=&#34;_blank&#34;&gt;Artifacia&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Winter was fast approaching, I had the strive to pursue my newfound love of machine learning, and some fortuitous circumstances led me here to Artifacia.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I had a humble background in Machine Learning and Computer Vision having done a research project and a bunch of online courses. It was an interesting selection process that got me through to becoming an intern. So let’s start with the exciting journey of learning to learn in India’s very own valley.&lt;/p&gt;

&lt;p&gt;My first day at the startup was nothing like I had expected (and I mean that in the best of ways possible). I was introduced, oriented, taught how things were done and I was loving the startup experience already. Me interestingly, a bit of an introvert opened up before I knew to the nice and hospitable team at Artifacia. Little did I know, that at the end of my intern I would feel like a part of the Artifacia family.&lt;/p&gt;

&lt;p&gt;I was told, I would get to make the choice of the projects that I’d be working on and the main motive of my task was to get a version 0.1 running. While I could get into the gory (but beautiful) details of my projects, I’d like to focus more on the journey here. I delved into my first project and soon understood how working at a startup was distinctly different from the academic research I was used to. The pace, to be honest, was daunting. But at the same time Navneet and Vivek kept the fire, so to say, burning with pep talks and discussions of exploring new projects and expanding the boundaries of AI. My biggest take-away from this in the process of getting v0.1 up and running was to learn quick but build and test faster.&lt;/p&gt;

&lt;p&gt;It might be banal to say that working on a startup is different but I would like to reiterate and say it helps you grow in ways you’d never imagined. I understood what it was to work with a team and how academic research was different from the real life implementation. I picked up the skill of wading through piles of research papers to grow further and soon reading research papers turned into a hobby. The team is what makes a startup great and the research team at Artifacia is what made it’s tech so. The Artifacia Research conversations remain one of my best memories discussing things from hallucinating GANs to creating humans dreaming because of built in GANs.&lt;/p&gt;

&lt;p&gt;To top it all up Artifacia outings and celebrations are something I’ll cherish for long. A christmas lunch, the secret santa,the AI meetups, the New Year Sci-Fi CosPlay, playing board games - everything, made my experience at Artifacia truly serendipitous.&lt;/p&gt;

&lt;p&gt;I finally got through the winters with four projects in five weeks and a built up resolve to learn and do that shall stay with me for long.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Improving Dynamic Streaming with Deep Learning</title>
      <link>kanishkg.github.io/project/predict-look-360/</link>
      <pubDate>Sun, 17 Dec 2017 17:17:05 +0530</pubDate>
      
      <guid>kanishkg.github.io/project/predict-look-360/</guid>
      <description>

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;Predicting the saliency of information for videos has been a popular problem, with uses in compression and streaming applications. Gaze predictions for future frames in 360 degree videos can enhance dynamic streaming capabilities providing greater pixels in the Field Of View of the viewer.&lt;/p&gt;

&lt;p&gt;A model to predict a probability distribution for the future second in real time is proposed using a combination of rudimentary trajectory prediction and a deep visual features. Further, measurement of the average hit rate is used to show efficiency of the model along with the Mean Squared Error.&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Saliency detection is originally a task of predicting scene locations where a human observer may fixate. Saliency detection has recently attracted a great amount of research interest.&lt;/p&gt;

&lt;p&gt;The reason behind this growing popularity lies in the effective use of these models in
various vision tasks, such as image segmentation, object detection, video summarization
and compression, to name a few. The growing popularity of 360 degree videos as the form
of entertainment and broadcast media has driven research in effectively streaming the
content.&lt;/p&gt;

&lt;p&gt;Predicting fixation probabilities of human viewers in the video can allow for us to send
the parts of the video with higher likelihood of being viewed with a higher quality. The
motive is to push the highest number of pixels into the field of view of the viewer. Thus,
improving the dynamic streaming capabilities.&lt;/p&gt;

&lt;p&gt;Saliency prediction for 2-d videos has received great attention, but the 360 degree counterpart is deprived of such efforts. Directly transferring models trained on 2-d videos proves challenging due to the spherical representation of the video and also throws away important information about the particular domain.&lt;/p&gt;

&lt;p&gt;In this Project, it is attempted to predict the viewpoints for future frames given the fixation trajectories for the past few seconds. Initially, we try exploring methods using visual features, but later the approach is shifted towards using rudimentary coordinates to get an initial estimate that can be refined to produce the final predictions.&lt;/p&gt;

&lt;p&gt;The major problem faced while learning visual features is the supervised setting that
requires a large amount of data and this domain suffers from a severe dearth of annotated
public datasets. Transfer learning features can be an option that seems promising but
proper adaptation of the architecture for a 360 degree setting is necessary. Also, the computation needs to be done on the fly, to make the algorithm useable in practical circumstances.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://drive.google.com/file/d/1W1_hWZQVC5h0cv8ZlXHIjlxA59rqFxSw/view&#34; target=&#34;_blank&#34;&gt;Read More&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variational Paraphrase Generation</title>
      <link>kanishkg.github.io/project/variational-paraphrase/</link>
      <pubDate>Sun, 17 Dec 2017 16:52:58 +0530</pubDate>
      
      <guid>kanishkg.github.io/project/variational-paraphrase/</guid>
      <description>

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;Among the most enticing things about natural
language is the fact that it can be written
and spoken in myriad ways. The act of
paraphrasing, or saying something that has the
same meaning as the original, but in a slightly
different manner, has a plethora of possible
applications, for e.g. sentence simplification,
complication, reporting, vocabulary modulation,
tone regulation. We provide a novel deep
generative model for paraphrase generation
using a variational auto-encoder. We also
suggest a method for controlled paraphrase
generation using pre-fixed latent variables.
We obtain promising results on machine
translation &amp;amp; paraphrase evaluation metrics
and some interesting correlations in human
evaluation. Our results are qualitatively and
quantitatively comparable. We also report
the current issues with paraphrase evaluation
methods, and analyze the popular datasets in
that respect.&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Spoken and written content can be formulated in
myriad ways to make it accessible to a diverse set
of audiences. A classic by Charles Dickens would
need its sentences to be expressed in a simpler
format for children to be able to understand it. On
the other hand, a children’s story would need to be
adapted in a more mature language with a wider
variety in sentence structure and vocabulary in
order to be appealing for adult readers. Reminders
and queries can be rephrased to make them seem
more actionable. Computer agents communicating
with humans can use multiple wordings
of the same intent to elicit the desired response
from the interacting person. These demands can
be fulfilled by an efficient paraphrase generation
system, if trained on a proper dataset.
Deep learning has found applications in a
variety of domains in the recent years. The
availability of massive datasets and plenty of
computational resources have been instrumental
in aiding deep learning algorithms to find solutions
to problems that were a long shot earlier.
Its influence on problems in natural language
processing has been immense as well, right from
state of the art sentiment classifiers and spreading
its net wide enough to entail impressive language
understanding and generation techniques. Paraphrase
generation is one of the many sequence to
sequence generation tasks that have been tackled
by deep learning. Although decent models exist,
much work remains before we can arrive at
industry grade paraphrasers that are expressive in
and adaptable to a wide range of domains.
Probabilistic modeling of classification tasks
has existed since decades, but its combination
with deep neural nets has led to a recent surge
in innovations leading up to deep generative
models. Variational Autoencoders (Kingma
and Welling, 2013) and Generative Adversarial
Networks (Goodfellow et al., 2014) have been
some of the more popular models. While their
utility and applications in natural language tasks
is being studied, a flurry of work on VAE modifications
to be adaptable to sequence to sequence
learning has appeared over the last couple of years.
Solving the problem of paraphrase generation
using variational autoencoders thus seemed like
an interesting idea for a project, and we went
ahead with it. The recent work by (Gupta et al.,
2017) has been a guiding light, since it has
tackled the same problem, though with a different
architecture.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://drive.google.com/file/d/10MSeoNFoZyvzqJPW75rCjw425SusIh6p/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;Read More&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
